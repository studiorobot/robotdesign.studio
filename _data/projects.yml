- id: 16
  title: "MobiPrint: A Mobile 3D Printer for Environment-Scale Design and Fabrication"
  authors: "Daniel Campos Zamora, Liang He, and Jon Froehlich"
  conference: "UIST 2024"
  pdf_link: "archive/mobiprint.pdf"
  acm_link: "https://doi.org/10.1145/3654777.3676459"
  video_link: "https://youtu.be/SknW-Oygh3w"
  code_link: "https://github.com/makeabilitylab/MobiPrint"
  poster: "research/mobiprint/thumbnail.jpg"
  video_webm: "research/mobiprint/mobiprint.webm"
  video_mp4: "research/mobiprint/mobiprint.mov"
  abstract: "We present MobiPrint, a prototype mobile fabrication system that combines elements from robotics, architecture, and Human-Computer Interaction (HCI) to enable environment-scale design and fabrication in ad-hoc indoor environments. MobiPrint provides a multi-stage fabrication pipeline: first, the robotic 3D printer automatically scans and maps an indoor space; second, a custom design tool converts the map into an interactive CAD canvas for editing and placing models in the physical world; finally, the MobiPrint robot prints the object directly on the ground at the defined location. Through a “proof-by-demonstration” validation, we highlight our system’s potential across different applications, including accessibility, home furnishing, floor signage, and art."

- id: 15
  title: "3D Printing Magnetophoretic Displays"
  authors: "Zeyu Yan, Hsuanling Lee, Liang He, and Huaishu Peng"
  conference: "UIST 2023"
  pdf_link: "archive/printed_display.pdf"
  acm_link: "https://doi.org/10.1145/3586183.3606804"
  video_link: "https://youtu.be/zzijtgQIkPY?si=UNF_c4ArqRXEs-SK"
  code_link: "https://github.com/zyyan20h/printDisplay"
  poster: "research/magdisplay/thumbnail.jpg"
  video_webm: "research/magdisplay/magdisplay.webm"
  video_mp4: "research/magdisplay/magdisplay.mov"
  abstract: "We present a pipeline for printing interactive and always-on magnetophoretic displays using affordable FDM 3D printers. Using our pipeline, an end-user can convert the surface of a 3D shape into a matrix of voxels. The generated model can be sent to an FDM 3D printer equipped with an additional syringe-based injector. During the printing process, an oil and iron powder-based liquid mixture is injected into each voxel cell, allowing the appearance of the once-printed object to be editable with external magnetic sources. To achieve this, we made modifications to the 3D printer hardware and the firmware. We also developed a 3D editor to prepare printable models. We demonstrate our pipeline with a variety of examples, including a printed Stanford bunny with customizable appearances, a small espresso mug that can be used as a post-it note surface, a board game figurine with a computationally updated display, and a collection of flexible wearable accessories with editable visuals."

- id: 13
  title: "Kinergy: Creating 3D Printable Motion using Embedded Kinetic Energy"
  authors: "Liang He, Xia Su, Huaishu Peng, Jeffrey I. Lipton, and Jon E. Froehlich"
  conference: "UIST 2022"
  pdf_link: "archive/kinergy.pdf"
  acm_link: "https://doi.org/10.1145/3526113.3545636"
  video_link: "https://youtu.be/keeWX802S1E"
  code_link: "https://github.com/makeabilitylab/Kinergy"
  poster: "research/kinergy/thumbnail.jpg"
  video_webm: "research/kinergy/kinery.webm"
  video_mp4: "research/kinergy/kinergy.mov"
  abstract: "We present <i>Kinergy</i>—an interactive design tool for creating self-propelled motion by harnessing the energy stored in 3D printable springs. To produce controllable output motions, we introduce 3D printable kinetic units, a set of parameterizable designs that encapsulate 3D printable springs, compliant locks, and transmission mechanisms for three non-periodic motions—instant translation, instant rotation, continuous translation—and four periodic motions—continuous rotation, reciprocation, oscillation, intermittent rotation. <i>Kinergy</i> allows the user to create motion-enabled 3D models by embedding kinetic units, customize output motion characteristics by parameterizing embedded springs and kinematic elements, control energy by operating the specialized lock, and preview the resulting motion in an interactive environment. We demonstrate the potential of our techniques via example applications from spring-loaded cars to kinetic sculptures and close with a discussion of key challenges such as geometric constraints."

- id: 12
  title: "FlexHaptics: A Design Method for Passive Haptic Inputs Using Planar Compliant Structures"
  authors: "Hongnan Lin, Liang He, Fangli Song, Yifan Li, Tingyu Cheng, Clement Zheng, Wei Wang, and Hyunjoo Oh"
  conference: "CHI 2022"
  pdf_link: "archive/flexhaptics.pdf"
  video_link: "https://youtu.be/GWFYzo-zAYM"
  code_link: "https://github.com/hlin0101/FlexHaptics"
  poster: "research/flexhaptics/thumbnail.jpg"
  video_webm: "research/flexhaptics/flexhaptics.webm"
  video_mp4: "research/flexhaptics/flexhaptics.mov"
  abstract: "This paper presents FlexHaptics, a design method for creating custom haptic input interfaces. Our approach leverages planar compliant structures whose force-deformation relationship can be altered by adjusting the geometries. Embedded with such structures, a FlexHaptics module exerts a fine-tunable haptic effect (<i>i.e.,</i> resistance, detent, or bounce) along a movement path (<i>i.e.,</i> linear, rotary, or ortho-planar). These modules can work separately or combine into an interface with complex movement paths and haptic effects. To enable the parametric design of FlexHaptic modules, we provide a design editor that converts user-specified haptic properties into underlying mechanical structures of haptic modules. We validate our approach and demonstrate the potential of FlexHaptic modules through six application examples, including a slider control for a painting application and a piano keyboard interface on touchscreens, a tactile low vision timer, VR game controllers, and a compound input device of a joystick and a two-step button."

- id: 11
  title: "ModElec: A Design Tool for Prototyping Physical Computing Devices Using Conductive 3D Printing"
  authors: "Liang He, Jarrid A Wittkopf, Ji Won Jun, Kris Erickson, and Rafael 'Tico' Ballagas"
  conference: "IMWUT 2021 / UbiComp 2022"
  pdf_link: "archive/modelec.pdf"
  video_link: "https://vimeo.com/679538148"
  code_link: "https://github.com/EdigaHe/modelec"
  poster: "research/modelec/thumbnail.jpg"
  video_webm: "research/modelec/modelec.webm"
  video_mp4: "research/modelec/modelec.mov"
  abstract: "Integrating electronics with highly custom 3D designs for the physical fabrication of interactive prototypes is traditionally cumbersome and requires numerous iterations of manual assembly and debugging. With the new capabilities of 3D printers, combining electronic design and 3D modeling workflows can lower the barrier for achieving interactive functionality or iterating on the overall design. We present ModElec—an interactive design tool that enables the coordinated expression of electronic and physical design intent by allowing designers to integrate 3D-printable circuits with 3D forms. With ModElec, the user can arrange electronic parts in a 3D body, modify the model design with embedded circuits updated, and preview the auto-generated 3D traces that can be directly printed with a multi-material-based 3D printer."

- id: 6
  title: "HulaMove: Using Commodity IMU for Waist Interaction"
  authors: "Xuhai Xu, Jiahao Li, Tianyi Yuan, Liang He, Xin Liu, Yukang Yan, Yuntao Wang, Yuanchun Shi, Jennifer Mankoff, and Anind K Dey."
  conference: "CHI 2021"
  pdf_link: "archive/hulamove.pdf"
  video_link: "https://youtu.be/doqZrZW8I4A"
  poster: "research/hulamove/thumbnail.jpg"
  video_webm: "research/hulamove/hulamove.webm"
  video_mp4: "research/hulamove/hulamove.mov"
  abstract: "We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings."

- id: 10
  title: "Ondulé: Designing and Controlling 3D Printable Springs"
  authors: "Liang He, Huaishu Peng, Michelle Lin, Ravikanth Konjeti, François Guimbretière, and Jon E. Froehlich"
  conference: "UIST 2019"
  pdf_link: "archive/ufp9144-he.pdf"
  video_link: "https://youtu.be/Zln1WlrDQ-4"
  code_link: "https://github.com/EdigaHe/Ondule/"
  poster: "research/ondule/thumbnail.jpg"
  video_webm: "research/ondule/ondule.webm"
  video_mp4: "research/ondule/ondule.mov"
  abstract: "We present <i>Ondulé</i>—an interactive design tool that allows novices to create parameterizable deformation behaviors in 3D-printable models using helical springs and embedded joints. Informed by spring theory and our empirical mechanical experiments, we introduce spring and joint-based design techniques that support a range of parameterizable deformation behaviors, including compress, extend, twist, bend, and various combinations. To enable users to design and add these deformations to their models, we introduce a custom design tool for Rhino. With the tool, users can convert selected geometries into springs, customize spring stiffness, and parameterize their design with mechanical constraints for desired behaviors."

- id: 9
  title: "SqueezaPulse: Adding Interactive Input to Fabricated Objects"
  authors: "Liang He, Gierad Laput, Eric Brockmeyer, and Jon E. Froehlich"
  conference: "TEI 2017"
  pdf_link: "archive/SqueezaPulse_CR_HighQualityPDF.pdf"
  video_link: "https://youtu.be/wmBz1dl1nC8"
  poster: "research/squeezapulse/thumbnail.jpg"
  video_webm: "research/squeezapulse/squeezapulse.webm"
  video_mp4: "research/squeezapulse/squeezapulse.mov"
  abstract: "We present SqueezaPulse, a technique for embedding interactivity into fabricated objects using soft, passive, lowcost bellow-like structures. When a soft cavity is squeezed, air pulses travel along a flexible pipe and into a uniquely designed corrugated tube that shapes the airflow into predictable sound signatures. A microphone captures and identifies these air pulses enabling interactivity. Informed by the underlying acoustic theory, we described an informal examination of the effect of different 3D-printed corrugations on air signatures and our resulting SqueezaPulse implementation. To demonstrate and evaluate the potential of SqueezaPulse, we present four prototype applications and a small, lab-based user study (N=9). Our evaluations show that our approach is accurate across users and robust to external noise."

- id: 8
  title: "MakerWear: A Tangible Approach to Interactive Wearable Creation"
  authors: "Majeed Kazemitabaar, Jason McPeak, Alexander Jiao, Liang He, Thomas Outing, and Jon E. Froehlich"
  conference: "CHI 2017"
  pdf_link: "archive/makerwear.pdf"
  video_link: "https://youtu.be/14Fa_VOJHIA"
  code_link: "https://github.com/MakerWear"
  poster: "research/makerwear/thumbnail.jpg"
  video_webm: "research/makerwear/makerwear.webm"
  video_mp4: "research/makerwear/makerwear.mov"
  abstract: "Wearable construction toolkits have shown promise in broadening participation in computing and empowering users to create personally meaningful computational designs. However, these kits present a high barrier of entry for some users, particularly young children (K-6). In this paper, we introduce MakerWear, a new wearable construction kit for children that uses a tangible, modular approach to wearable creation. We describe our participatory design process, the iterative development of MakerWear, and results from single- and multi-session workshops with 32 children (ages 5-12; M=8.3 years). Our findings reveal how children engage in wearable design, what they make (and want to make), and what challenges they face. As a secondary analysis, we also explore age-related differences."
  awards:
  - title: "Best Paper Award at CHI'17"
    year: "CHI'17"
  - title: "Best LBW Paper Award at CHI'16"
    year: "CHI'16"

- id: 7
  title: "New Interaction Tools for Preserving an Old Language"
  authors: "Beryl Plimmer, Liang He, Tariq Zaman, Kasun Karunanayaka, Alvin W. Yeo, Garen Jengan, Rachel Blagojevic, and Ellen Yi-Luen Do"
  conference: "CHI 2015"
  pdf_link: "archive/tangibles.pdf"
  video_link: "https://youtu.be/tfj6G1XoMEQ"
  poster: "research/tangibles/thumbnail.jpg"
  video_webm: "research/tangibles/tangibles.webm"
  video_mp4: "research/tangibles/tangibles.mov"
  abstract: "The Penan people of Malaysian Borneo were traditionally nomads of the rainforest. They would leave messages in the jungle for each other by shaping natural objects into language tokens and arranging these symbols in specific ways -- much like words in a sentence. With settlement, the language is being lost as it is not being used by the younger generation. We report here, a tangible system designed to help the Penan preserve their unique object writing language. The key features of the system are that: its tangibles are made of real objects; it works in the wild; and new tangibles can be fabricated and added to the system by the users. Our evaluations show that the system is engaging and encourages intergenerational knowledge transfer and thus has the potential to help preserve this language."
  honorable_mentions:
  - title: "Honorable Mentions Award at CHI'15"
    year: "CHI'15"
    
- id: 2
  title: "CozyMaps: Real-time Collaboration With Multiple Displays"
  authors: "Kelvin Cheng, Liang He, Xiaojun Meng, David A. Shamma, Dung Nguyen, and Anbarasan T."
  conference: "MobileHCI 2015"
  pdf_link: "archive/cozymaps.pdf"
  video_link: "https://youtu.be/IMq4K9zeE54"
  poster: "research/cozymaps/thumbnail.jpg"
  video_webm: "research/cozymaps/cozymaps.webm"
  video_mp4: "research/cozymaps/cozymaps.mov"
  abstract: "With the use of several tablet devices and a shared large display, CozyMaps is a multi-display system that supports real-time collocated collaboration on a shared map. This paper builds on existing works and introduces rich user interactions by proposing awareness, notification, and view sharing techniques, to enable seamless information sharing and integration in map-based applications. Based on our exploratory study, we demonstrated that participants are satisfied with these new proposed interactions. We found that view sharing techniques should be location-focused rather than user-focused. Our results provide implications for the design of interactive techniques in collaborative multi-display map systems."
